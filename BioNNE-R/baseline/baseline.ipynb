{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BioNNE-R Relation Extraction Baseline\n\nInteractive notebook version of the OpenNRE-based baseline for relation extraction using `bert-base-multilingual-cased` with entity markers.\n\nSupports three modes:\n1. **Labeled mode** — relation TSV to JSON lines (standard training/evaluation)\n2. **Labeled + negative sampling** — adds `no_relation` negatives from entity inventory\n3. **Blind mode** — entity TSV to all candidate pairs (for blind evaluation)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies if needed\n# !pip install git+https://github.com/thunlp/OpenNRE.git\n# !pip install torch transformers nltk pandas scikit-learn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport random\nimport re\nfrom collections import Counter\nfrom pathlib import Path\n\nimport nltk\nimport opennre\nimport pandas as pd\nimport torch\n\ntry:\n    nltk.data.find(\"tokenizers/punkt_tab\")\nexcept LookupError:\n    nltk.download(\"punkt_tab\", quiet=True)\n\nRELATION_TYPES = [\n    \"ABBREVIATION\", \"ALTERNATIVE_NAME\", \"SUBCLASS_OF\", \"PART_OF\",\n    \"TREATED_USING\", \"ORIGINS_FROM\", \"TO_DETECT_OR_STUDY\", \"AFFECTS\",\n    \"HAS_CAUSE\", \"APPLIED_TO\", \"USED_IN\", \"ASSOCIATED_WITH\",\n    \"PHYSIOLOGY_OF\", \"FINDING_OF\",\n    \"no_relation\",\n]\n\nprint(f\"Relation classes: {len(RELATION_TYPES)} (including no_relation)\")\nprint(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration — adjust paths for your setup\nMODEL_NAME = \"bert-base-multilingual-cased\"\nMAX_LENGTH = 256\nBATCH_SIZE = 16\nLEARNING_RATE = 2e-5\nEPOCHS = 10\nWARMUP_STEPS = 300\nLANG = \"english\"\nSEED = 42\n\n# Paths\nTRAIN_REL_TSV = Path(\"eng-train-rel.tsv\")      # relation TSV for training\nTRAIN_ENT_TSV = Path(\"eng-train-ent.tsv\")       # entity TSV for negative sampling\nDEV_REL_TSV = Path(\"eng-dev-rel.tsv\")           # relation TSV for dev\nTEXTS_DIR = Path(\"texts\")                        # raw .txt article files\nCONFIG_PATH = Path(\"../data/annotation_short-bio.conf\")  # annotation config for type filtering\nNEG_RATIO = 3                                    # negatives per positive (0 = no negatives)\n\nOUTPUT_DIR = Path(\"data\")\nCKPT_PATH = Path(\"outputs/model.pth.tar\")\nREL2ID_PATH = OUTPUT_DIR / \"rel2id.json\"\n\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nPath(\"outputs\").mkdir(parents=True, exist_ok=True)\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Neg ratio: {NEG_RATIO}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Preparation Functions\n\nAll functions from `prepare_data.py` — config parsing, entity loading, pair generation, sentence segmentation, negative sampling, blind mode conversion."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_config(config_path):\n    \"\"\"Parse annotation config and return set of valid (arg1_type, arg2_type) tuples.\"\"\"\n    valid_pairs = set()\n    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n    relations_match = re.search(r\"\\[relations\\](.*?)(?=\\[|\\Z)\", content, re.DOTALL)\n    if not relations_match:\n        return valid_pairs\n    for line in relations_match.group(1).strip().split(\"\\n\"):\n        line = line.strip()\n        if not line or line.startswith(\"<\"):\n            continue\n        parts = line.split(None, 1)\n        if len(parts) < 2:\n            continue\n        args_part = parts[1]\n        arg1_match = re.search(r\"Arg1:([A-Z_|]+)\", args_part)\n        if not arg1_match:\n            continue\n        arg1_types = arg1_match.group(1).split(\"|\")\n        arg2_match = re.search(r\"Arg2:([A-Z_|]+)\", args_part)\n        if arg2_match:\n            arg2_types = arg2_match.group(1).split(\"|\")\n        else:\n            comma_match = re.search(r\",\\s*([A-Z_|]+)\\s*$\", args_part)\n            if comma_match:\n                arg2_types = comma_match.group(1).split(\"|\")\n            else:\n                continue\n        for t1 in arg1_types:\n            for t2 in arg2_types:\n                valid_pairs.add((t1, t2))\n    return valid_pairs\n\n\ndef load_texts(texts_dir):\n    \"\"\"Load all .txt files from directory into {doc_id: text} dict.\"\"\"\n    texts = {}\n    for txt_file in Path(texts_dir).glob(\"*.txt\"):\n        texts[txt_file.stem] = txt_file.read_text(encoding=\"utf-8\")\n    return texts\n\n\ndef load_entities(entity_tsv):\n    \"\"\"Read entity TSV -> {doc_id: [(type, text, span), ...]}.\"\"\"\n    df = pd.read_csv(entity_tsv, sep=\"\\t\")\n    entities_by_doc = {}\n    for _, row in df.iterrows():\n        doc_id = str(row[\"document_id\"])\n        entry = (row[\"entity_type\"], str(row[\"entity_text\"]), str(row[\"entity_span\"]))\n        entities_by_doc.setdefault(doc_id, []).append(entry)\n    return entities_by_doc\n\n\ndef parse_span(span_str):\n    \"\"\"Parse 'start-end' to (start, end).\"\"\"\n    start, end = span_str.split(\"-\")\n    return int(start), int(end)\n\n\ndef generate_pairs(entities, valid_type_pairs=None):\n    \"\"\"Generate ordered entity pairs, optionally filtered by valid type combinations.\"\"\"\n    pairs = []\n    for i, e1 in enumerate(entities):\n        for j, e2 in enumerate(entities):\n            if i == j:\n                continue\n            if valid_type_pairs is not None and (e1[0], e2[0]) not in valid_type_pairs:\n                continue\n            pairs.append((e1, e2))\n    return pairs\n\n\ndef find_sentence_segment(text, head_start, head_end, tail_start, tail_end, lang=\"english\"):\n    \"\"\"Find minimal sentence segment containing both entities. Returns (segment_text, offset).\"\"\"\n    try:\n        sent_tokenizer = nltk.data.load(f\"tokenizers/punkt_tab/{lang}.pickle\")\n    except LookupError:\n        sent_tokenizer = nltk.data.load(\"tokenizers/punkt_tab/english.pickle\")\n    sentences = list(sent_tokenizer.span_tokenize(text))\n    if not sentences:\n        return text, 0\n    entity_min = min(head_start, tail_start)\n    entity_max = max(head_end, tail_end)\n    first_idx = 0\n    last_idx = len(sentences) - 1\n    for i, (s_start, s_end) in enumerate(sentences):\n        if s_start <= entity_min < s_end:\n            first_idx = i\n        if s_start < entity_max <= s_end:\n            last_idx = i\n    if first_idx > last_idx:\n        first_idx, last_idx = last_idx, first_idx\n    first_idx = max(0, first_idx - 1)\n    last_idx = min(len(sentences) - 1, last_idx + 1)\n    seg_start = sentences[first_idx][0]\n    seg_end = sentences[last_idx][1]\n    return text[seg_start:seg_end], seg_start\n\n\ndef _make_instance(text, head_type, head_text, head_span, tail_type, tail_text, tail_span, relation, doc_id, lang):\n    \"\"\"Build one OpenNRE JSON instance from entity pair info.\"\"\"\n    head_start, head_end = parse_span(head_span)\n    tail_start, tail_end = parse_span(tail_span)\n    segment, offset = find_sentence_segment(text, head_start, head_end, tail_start, tail_end, lang)\n    h_start = head_start - offset\n    h_end = head_end - offset\n    t_start = tail_start - offset\n    t_end = tail_end - offset\n    if h_start < 0 or h_end > len(segment) or t_start < 0 or t_end > len(segment):\n        segment = text\n        h_start, h_end = head_start, head_end\n        t_start, t_end = tail_start, tail_end\n    return {\n        \"text\": segment,\n        \"h\": {\"name\": head_text, \"pos\": [h_start, h_end]},\n        \"t\": {\"name\": tail_text, \"pos\": [t_start, t_end]},\n        \"relation\": relation,\n        \"doc_id\": doc_id,\n        \"head_span\": head_span,\n        \"tail_span\": tail_span,\n        \"head_type\": head_type,\n        \"tail_type\": tail_type,\n    }\n\n\ndef add_negatives(positives_by_doc, entities_by_doc, neg_ratio, valid_type_pairs, seed):\n    \"\"\"Sample negative pairs per document.\"\"\"\n    rng = random.Random(seed)\n    negatives_by_doc = {}\n    for doc_id, entities in entities_by_doc.items():\n        pos_set = positives_by_doc.get(doc_id, set())\n        all_pairs = generate_pairs(entities, valid_type_pairs)\n        neg_candidates = [(e1, e2) for e1, e2 in all_pairs if (e1[2], e2[2]) not in pos_set]\n        n_positives = len(pos_set)\n        n_sample = min(neg_ratio * n_positives, len(neg_candidates))\n        if n_sample > 0:\n            negatives_by_doc[doc_id] = rng.sample(neg_candidates, n_sample)\n    return negatives_by_doc\n\n\ndef convert_split(rel_tsv, texts_dir, output_path, lang=\"english\",\n                  entities_tsv=None, neg_ratio=0, valid_type_pairs=None, seed=42):\n    \"\"\"Convert labeled relation TSV + texts to OpenNRE JSON lines. Returns instance count.\"\"\"\n    df = pd.read_csv(rel_tsv, sep=\"\\t\")\n    texts = load_texts(texts_dir)\n    entities_by_doc = None\n    positives_by_doc = {}\n    if entities_tsv and neg_ratio > 0:\n        entities_by_doc = load_entities(entities_tsv)\n        for _, row in df.iterrows():\n            doc_id = str(row[\"document_id\"])\n            positives_by_doc.setdefault(doc_id, set()).add(\n                (str(row[\"head_span\"]), str(row[\"tail_span\"]))\n            )\n    count = 0\n    neg_count = 0\n    skipped = 0\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        for _, row in df.iterrows():\n            doc_id = str(row[\"document_id\"])\n            relation = row[\"relation\"]\n            if doc_id not in texts:\n                skipped += 1\n                continue\n            if relation not in RELATION_TYPES:\n                skipped += 1\n                continue\n            instance = _make_instance(\n                texts[doc_id], row[\"head_type\"], row[\"head_text\"], str(row[\"head_span\"]),\n                row[\"tail_type\"], row[\"tail_text\"], str(row[\"tail_span\"]),\n                relation, doc_id, lang,\n            )\n            if instance:\n                f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n                count += 1\n        if entities_by_doc and neg_ratio > 0:\n            neg_pairs_by_doc = add_negatives(positives_by_doc, entities_by_doc, neg_ratio, valid_type_pairs, seed)\n            for doc_id, neg_pairs in neg_pairs_by_doc.items():\n                if doc_id not in texts:\n                    continue\n                text = texts[doc_id]\n                for head_ent, tail_ent in neg_pairs:\n                    instance = _make_instance(\n                        text, head_ent[0], head_ent[1], head_ent[2],\n                        tail_ent[0], tail_ent[1], tail_ent[2],\n                        \"no_relation\", doc_id, lang,\n                    )\n                    if instance:\n                        f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n                        neg_count += 1\n    if skipped:\n        print(f\"  Skipped {skipped} instances (missing text or unknown relation)\")\n    if neg_count:\n        print(f\"  Added {neg_count} no_relation negatives (ratio {neg_ratio}:1)\")\n    return count + neg_count\n\n\ndef convert_blind(entity_tsv, texts_dir, output_path, valid_type_pairs=None, lang=\"english\"):\n    \"\"\"Blind mode: generate all candidate pairs from entity TSV. Returns instance count.\"\"\"\n    entities_by_doc = load_entities(entity_tsv)\n    texts = load_texts(texts_dir)\n    count = 0\n    skipped_docs = 0\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        for doc_id, entities in sorted(entities_by_doc.items()):\n            if doc_id not in texts:\n                skipped_docs += 1\n                continue\n            text = texts[doc_id]\n            pairs = generate_pairs(entities, valid_type_pairs)\n            for head_ent, tail_ent in pairs:\n                instance = _make_instance(\n                    text, head_ent[0], head_ent[1], head_ent[2],\n                    tail_ent[0], tail_ent[1], tail_ent[2],\n                    \"no_relation\", doc_id, lang,\n                )\n                if instance:\n                    f.write(json.dumps(instance, ensure_ascii=False) + \"\\n\")\n                    count += 1\n    if skipped_docs:\n        print(f\"  Skipped {skipped_docs} documents (no matching text file)\")\n    return count\n\n\ndef detect_input_type(tsv_path):\n    \"\"\"Auto-detect TSV type by column names. Returns 'relation' or 'entity'.\"\"\"\n    df = pd.read_csv(tsv_path, sep=\"\\t\", nrows=0)\n    columns = set(df.columns)\n    if \"relation\" in columns:\n        return \"relation\"\n    if \"entity_type\" in columns:\n        return \"entity\"\n    raise ValueError(f\"Cannot auto-detect TSV type from columns: {sorted(columns)}\")\n\n\nprint(\"Data preparation functions loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate rel2id.json (always includes no_relation as class 14)\nrel2id = {rel: i for i, rel in enumerate(RELATION_TYPES)}\nwith open(REL2ID_PATH, \"w\") as f:\n    json.dump(rel2id, f, indent=2)\nprint(f\"Wrote {REL2ID_PATH} ({len(rel2id)} classes)\")\n\n# Load config for type-based pair filtering (optional)\nvalid_type_pairs = None\nif CONFIG_PATH.exists():\n    valid_type_pairs = parse_config(str(CONFIG_PATH))\n    print(f\"Loaded config: {len(valid_type_pairs)} valid type pairs\")\nelse:\n    print(f\"Config not found at {CONFIG_PATH}, skipping type filtering\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare training data (labeled + negative sampling)\ntrain_output = OUTPUT_DIR / \"train.txt\"\n\nprint(f\"Converting {TRAIN_REL_TSV} + {TEXTS_DIR} -> {train_output}\")\nn_train = convert_split(\n    TRAIN_REL_TSV, TEXTS_DIR, train_output,\n    lang=LANG,\n    entities_tsv=TRAIN_ENT_TSV if TRAIN_ENT_TSV.exists() else None,\n    neg_ratio=NEG_RATIO,\n    valid_type_pairs=valid_type_pairs,\n    seed=SEED,\n)\nprint(f\"  {n_train} total instances\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare dev data (labeled, no negatives)\ndev_output = OUTPUT_DIR / \"dev.txt\"\n\nprint(f\"Converting {DEV_REL_TSV} + {TEXTS_DIR} -> {dev_output}\")\nn_dev = convert_split(DEV_REL_TSV, TEXTS_DIR, dev_output, lang=LANG)\nprint(f\"  {n_dev} instances\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Data Exploration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and inspect prepared training data\ntrain_instances = []\nwith open(train_output, encoding=\"utf-8\") as f:\n    for line in f:\n        line = line.strip()\n        if line:\n            train_instances.append(json.loads(line))\n\n# Relation distribution\nrel_counts = Counter(inst[\"relation\"] for inst in train_instances)\nprint(f\"Training instances: {len(train_instances)}\")\nprint(f\"\\nRelation distribution:\")\nfor rel, count in rel_counts.most_common():\n    print(f\"  {rel:<25} {count:>6}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample instance\nsample = train_instances[0]\nprint(\"Sample instance:\")\nfor key, value in sample.items():\n    if key == \"text\":\n        print(f\"  {key}: {value[:200]}...\")\n    else:\n        print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load dev instances for later evaluation\ndev_instances = []\nwith open(dev_output, encoding=\"utf-8\") as f:\n    for line in f:\n        line = line.strip()\n        if line:\n            dev_instances.append(json.loads(line))\n\ndev_rel_counts = Counter(inst[\"relation\"] for inst in dev_instances)\nprint(f\"Dev instances: {len(dev_instances)}\")\nprint(f\"\\nDev relation distribution:\")\nfor rel, count in dev_rel_counts.most_common():\n    print(f\"  {rel:<25} {count:>6}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 4. Training"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Setup and train using OpenNRE SentenceRE framework"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Model: {MODEL_NAME}\")\nprint(f\"Train: {train_output}\")\nprint(f\"Dev: {dev_output}\")\nprint(f\"Classes: {len(rel2id)}\")\nprint(f\"Checkpoint: {CKPT_PATH}\")\n\nencoder = opennre.encoder.BERTEntityEncoder(\n    max_length=MAX_LENGTH,\n    pretrain_path=MODEL_NAME,\n)\n\nmodel = opennre.model.SoftmaxNN(\n    sentence_encoder=encoder,\n    num_class=len(rel2id),\n    rel2id=rel2id,\n)\n\nframework = opennre.framework.SentenceRE(\n    model=model,\n    train_path=str(train_output),\n    val_path=str(dev_output),\n    test_path=str(dev_output),\n    ckpt=str(CKPT_PATH),\n    batch_size=BATCH_SIZE,\n    max_epoch=EPOCHS,\n    lr=LEARNING_RATE,\n    opt=\"adamw\",\n    warmup_step=WARMUP_STEPS,\n)\n\nprint(\"Framework ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the model\nframework.train_model(metric=\"micro_f1\")\nprint(f\"\\nBest checkpoint saved to: {CKPT_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 5. Prediction on Dev Set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best checkpoint for prediction\nencoder_pred = opennre.encoder.BERTEntityEncoder(\n    max_length=MAX_LENGTH,\n    pretrain_path=MODEL_NAME,\n)\n\nmodel_pred = opennre.model.SoftmaxNN(\n    sentence_encoder=encoder_pred,\n    num_class=len(rel2id),\n    rel2id=rel2id,\n)\n\nckpt = torch.load(str(CKPT_PATH), map_location=\"cpu\")\nmodel_pred.load_state_dict(ckpt[\"state_dict\"])\n\nif torch.cuda.is_available():\n    model_pred = model_pred.cuda()\nmodel_pred.eval()\nprint(\"Model loaded for prediction.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Predict on dev set\nprint(f\"Predicting {len(dev_instances)} instances...\")\n\nrows = []\nfor inst in dev_instances:\n    pred_rel, score = model_pred.infer({\n        \"text\": inst[\"text\"],\n        \"h\": {\"pos\": inst[\"h\"][\"pos\"]},\n        \"t\": {\"pos\": inst[\"t\"][\"pos\"]},\n    })\n    rows.append({\n        \"document_id\": inst[\"doc_id\"],\n        \"relation\": pred_rel,\n        \"head_text\": inst[\"h\"][\"name\"],\n        \"head_span\": inst[\"head_span\"],\n        \"head_type\": inst[\"head_type\"],\n        \"tail_text\": inst[\"t\"][\"name\"],\n        \"tail_span\": inst[\"tail_span\"],\n        \"tail_type\": inst[\"tail_type\"],\n    })\n\n# Filter out no_relation predictions\npred_df = pd.DataFrame(rows)\ntotal_pred = len(pred_df)\npred_df = pred_df[pred_df[\"relation\"] != \"no_relation\"]\nfiltered = total_pred - len(pred_df)\nprint(f\"Filtered {filtered} no_relation predictions, {len(pred_df)} relations remaining\")\n\n# Save predictions\npred_path = Path(\"outputs/pred.tsv\")\npred_df.to_csv(pred_path, sep=\"\\t\", index=False)\nprint(f\"Predictions saved to: {pred_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-relation precision / recall / F1 (against gold labels in dev data)\nall_relations = sorted(rel2id.keys())\n\ncorrect = sum(1 for inst, row in zip(dev_instances, rows) if inst[\"relation\"] == row[\"relation\"])\ntotal = len(dev_instances)\nprint(f\"Accuracy: {correct}/{total} = {correct/total:.4f}\")\n\ngold_counts = Counter(inst[\"relation\"] for inst in dev_instances)\npred_counts = Counter(row[\"relation\"] for row in rows)\ntp_counts = Counter()\nfor inst, row in zip(dev_instances, rows):\n    if inst[\"relation\"] == row[\"relation\"]:\n        tp_counts[inst[\"relation\"]] += 1\n\nprint(f\"\\n{'Relation':<25} {'P':>8} {'R':>8} {'F1':>8} {'Support':>8}\")\nprint(\"-\" * 60)\nf1_scores = []\nfor rel in all_relations:\n    tp = tp_counts.get(rel, 0)\n    pred_total = pred_counts.get(rel, 0)\n    gold_total = gold_counts.get(rel, 0)\n    p = tp / pred_total if pred_total else 0\n    r = tp / gold_total if gold_total else 0\n    f1 = 2 * p * r / (p + r) if (p + r) else 0\n    if gold_total > 0:\n        f1_scores.append(f1)\n    print(f\"{rel:<25} {p:>8.4f} {r:>8.4f} {f1:>8.4f} {gold_total:>8}\")\n\nmacro_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\nprint(f\"\\nMacro F1: {macro_f1:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 7. Blind Prediction\n\nGenerate candidate pairs from entity TSV (no gold relations), predict, and filter `no_relation`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare blind test data from entity TSV\n# Change this path to your blind test entity TSV\nBLIND_ENT_TSV = Path(\"eng-test-ent.tsv\")\nBLIND_TEXTS_DIR = TEXTS_DIR  # same text directory, or change for test set\nblind_output = OUTPUT_DIR / \"test.txt\"\n\nif BLIND_ENT_TSV.exists():\n    print(f\"Blind mode: {BLIND_ENT_TSV} + {BLIND_TEXTS_DIR} -> {blind_output}\")\n    n_blind = convert_blind(\n        BLIND_ENT_TSV, BLIND_TEXTS_DIR, blind_output,\n        valid_type_pairs=valid_type_pairs,\n        lang=LANG,\n    )\n    print(f\"  {n_blind} candidate pairs\")\nelse:\n    print(f\"Blind entity TSV not found at {BLIND_ENT_TSV}, skipping.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run blind prediction (if test data was prepared)\nblind_pred_path = Path(\"outputs/blind_pred.tsv\")\n\nif blind_output.exists() and blind_output.stat().st_size > 0:\n    blind_instances = []\n    with open(blind_output, encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                blind_instances.append(json.loads(line))\n\n    print(f\"Predicting {len(blind_instances)} candidate pairs...\")\n\n    blind_rows = []\n    for inst in blind_instances:\n        pred_rel, score = model_pred.infer({\n            \"text\": inst[\"text\"],\n            \"h\": {\"pos\": inst[\"h\"][\"pos\"]},\n            \"t\": {\"pos\": inst[\"t\"][\"pos\"]},\n        })\n        blind_rows.append({\n            \"document_id\": inst[\"doc_id\"],\n            \"relation\": pred_rel,\n            \"head_text\": inst[\"h\"][\"name\"],\n            \"head_span\": inst[\"head_span\"],\n            \"head_type\": inst[\"head_type\"],\n            \"tail_text\": inst[\"t\"][\"name\"],\n            \"tail_span\": inst[\"tail_span\"],\n            \"tail_type\": inst[\"tail_type\"],\n        })\n\n    # Filter out no_relation\n    blind_pred_df = pd.DataFrame(blind_rows)\n    total_blind = len(blind_pred_df)\n    blind_pred_df = blind_pred_df[blind_pred_df[\"relation\"] != \"no_relation\"]\n    blind_filtered = total_blind - len(blind_pred_df)\n    print(f\"Filtered {blind_filtered} no_relation predictions, {len(blind_pred_df)} relations remaining\")\n\n    blind_pred_df.to_csv(blind_pred_path, sep=\"\\t\", index=False)\n    print(f\"Blind predictions saved to: {blind_pred_path}\")\nelse:\n    print(\"No blind test data to predict on.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Error Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find misclassified examples on dev set\nerrors = []\nfor inst, row in zip(dev_instances, rows):\n    if inst[\"relation\"] != row[\"relation\"]:\n        errors.append({\n            \"doc_id\": inst[\"doc_id\"],\n            \"head_text\": inst[\"h\"][\"name\"],\n            \"tail_text\": inst[\"t\"][\"name\"],\n            \"gold\": inst[\"relation\"],\n            \"predicted\": row[\"relation\"],\n            \"text\": inst[\"text\"][:200],\n        })\n\nprint(f\"Total errors: {len(errors)} / {len(rows)} ({100*len(errors)/len(rows):.1f}%)\")\nprint(f\"\\nFirst 5 errors:\")\nfor i, err in enumerate(errors[:5]):\n    print(f\"\\n--- Error {i+1} ---\")\n    print(f\"Doc: {err['doc_id']}\")\n    print(f\"Head: {err['head_text']}\")\n    print(f\"Tail: {err['tail_text']}\")\n    print(f\"Gold: {err['gold']}\")\n    print(f\"Predicted: {err['predicted']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix for most common relations\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n# Map relations to indices for confusion matrix\ngold_indices = [rel2id.get(inst[\"relation\"], -1) for inst in dev_instances]\npred_indices = [rel2id.get(row[\"relation\"], -1) for row in rows]\n\ncm = confusion_matrix(gold_indices, pred_indices, labels=list(range(len(RELATION_TYPES))))\n\n# Get top 8 most common relations by gold count\nlabel_counts = np.array([cm[i].sum() for i in range(len(RELATION_TYPES))])\ntop_labels = np.argsort(label_counts)[::-1][:8]\n\ncm_subset = cm[np.ix_(top_labels, top_labels)]\nlabels_subset = [RELATION_TYPES[i] for i in top_labels]\n\nplt.figure(figsize=(10, 8))\nplt.imshow(cm_subset, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix (Top 8 Relations)\")\nplt.colorbar()\nplt.xticks(range(len(labels_subset)), labels_subset, rotation=45, ha=\"right\")\nplt.yticks(range(len(labels_subset)), labels_subset)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}